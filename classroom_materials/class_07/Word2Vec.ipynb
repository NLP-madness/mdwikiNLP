{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('NLP')",
   "display_name": "Python 3.8.5 64-bit ('NLP')",
   "metadata": {
    "interpreter": {
     "hash": "2136a9c3637fd160483224d7922e48bf03b650be5dff26724a0c1f8d1279953b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1) Question for the videos?\n",
    "\n",
    "## 2) Homework questions\n",
    "- What is a word embedding?\n",
    "- What are the desired properties of word embeddings?\n",
    "- What are the potential uses of word embeddings?\n",
    "\n",
    "## 3) Validation of Word Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "path = os.path.join(\"..\",  # go back a folder\n",
    "                   \"class_06\") # go into class 6\n",
    "sys.path.append(path)  # add to path (so we can load the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['This', 'is', 'a', 'sample', 'text'], ['We', 'will', 'need', 'to', 'split', 'it', 'into', 'sentences'], ['Afterwards', ',', 'it', 'will', 'be', 'split', 'into', 'tokens']]\n"
     ]
    }
   ],
   "source": [
    "# example of preprocessing\n",
    "from text_preprocessor import Text\n",
    "\n",
    "text_data = \"This is a sample text. We will need to split it into sentences. Afterwards, it will be split into tokens\"\n",
    "\n",
    "text = Text(text_data)\n",
    "text.tokenize(method=\"nltk\", split_sent=True)\n",
    "print(text.get_tokens())\n",
    "\n",
    "sentences = text.get_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.1649963   0.03560992  0.12827325]\n"
     ]
    }
   ],
   "source": [
    "# train a word embedding \n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, \n",
    "                 size=3, # size of the embedding layer (very low!)\n",
    "                 window=5, # the size of the window (max distance between current and predicted word)\n",
    "                 min_count=1, # ignore word with freq lower than this\n",
    "                 sg = 1, # should it use skip-gram (alternative is CBOW)\n",
    "                 workers=4) # number of cores to to use when training\n",
    "\n",
    "print(model[\"sample\"])"
   ]
  },
  {
   "source": [
    "The word vector we can also we illustrated using color. Something like this:\n",
    "\n",
    "![](http://jalammar.github.io/images/word2vec/word2vec.png)\n",
    "\n",
    "\n",
    "---\n",
    "Well given the above example is just an example of how you would explain the differences. There is a lot of examples in English online so I will show you a Danish example here:\n",
    "\n",
    "(these can be loaded from DaNLP [github site](https://github.com/alexandrainst/danlp/blob/master/docs/models/embeddings.md))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "\n",
    "from danlp.models.embeddings  import load_wv_with_gensim\n",
    "\n",
    "if \"word2vec.model\" not in os.listdir():\n",
    "    word_embeddings = load_wv_with_gensim('conll17.da.wv')\n",
    "    word_embeddings.save(\"word2vec.model\")\n",
    "word_embeddings = KeyedVectors.load(\"word2vec.model\")"
   ]
  },
  {
   "source": [
    "## Similar Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Aarhus = \n",
      "[('aalborg', 0.8658724427223206), ('århus', 0.8577944040298462), ('københavns', 0.8440234661102295), ('odense', 0.8249165415763855), ('risskov', 0.80350661277771), ('kolding', 0.7970957159996033), ('roskilde', 0.793404221534729), ('herning', 0.7862382531166077), ('konferencer8000', 0.7729060649871826), ('amager', 0.7698875069618225)]\n",
      "\n",
      "Kat = \n",
      "[('hund', 0.8387212753295898), ('hamster', 0.8017323613166809), ('kanin', 0.7978731989860535), ('racekat', 0.7938638925552368), ('hunhund', 0.7916580438613892), ('hund.', 0.78998863697052), ('undulat', 0.7851536273956299), ('hvalp', 0.7825443744659424), ('hunden', 0.7818223237991333), ('katten', 0.7807555794715881)]\n",
      "\n",
      "is corona more associated with beer or a virus:\n",
      "0.24333946\n",
      "0.0184646\n"
     ]
    }
   ],
   "source": [
    "# most similar word (synonym detection)\n",
    "\n",
    "print(\"Aarhus = \")\n",
    "print(word_embeddings.most_similar(positive=['aarhus'], topn=10))\n",
    "\n",
    "print(\"\\nKat = \")\n",
    "print(word_embeddings.most_similar(positive=['kat'], topn=10))\n",
    "\n",
    "print(\"\\nis corona more associated with beer or a virus:\")\n",
    "print(word_embeddings.similarity('corona', 'øl'))\n",
    "print(word_embeddings.similarity('corona', 'virus'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('michael', 0.8561058044433594), ('jesper', 0.8532627820968628), ('kasper', 0.8431500196456909), ('lasse', 0.8396196365356445), ('allan', 0.8374769687652588), ('morten', 0.8355758190155029), ('madsen', 0.8354624509811401), ('peter', 0.8318194150924683), ('christensen', 0.8301103711128235), ('nielsen', 0.823228657245636)]\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings.most_similar(positive=['kenneth'], topn=10))"
   ]
  },
  {
   "source": [
    "## Analogies\n",
    "\"Copenhagen is to Denmark what ___ is to England\" \n",
    "\n",
    "or mathemathically: $CPH - DK + UK \\approx $ ?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('london', 0.7156291604042053), ('edinburgh', 0.6790332794189453), ('woolwich', 0.6561343669891357), ('eh1', 0.6467728614807129), ('leeds', 0.6460204124450684)]\n[('antalya', 0.678152322769165), ('istanbul', 0.6532646417617798), ('ankara', 0.6456939578056335), ('alanya', 0.636702299118042), ('izmir', 0.6343185901641846)]\n[('lægen', 0.7816711664199829), ('speciallæge', 0.7328570485115051), ('radiodoktor', 0.7276281714439392), ('ventegodt', 0.7276049852371216), ('flytlies', 0.7271700501441956)]\n"
     ]
    }
   ],
   "source": [
    "#capital\n",
    "print(word_embeddings.most_similar(positive=['københavn', 'england'], negative=['danmark'], topn=5))\n",
    "# maybe not?\n",
    "print(word_embeddings.most_similar(positive=['københavn', 'tyrkiet'], negative=['danmark'], topn=5))\n",
    "# conjugations\n",
    "print(word_embeddings.most_similar(positive=['læge', 'manden'], negative=['mand'], topn=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('monssen', 0.5216814875602722), ('reysen', 0.5103859901428223), ('forpagterne', 0.5088638067245483), ('øhlet', 0.5065404176712036), ('nicolls', 0.5060608386993408)]\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings.most_similar(positive=['mand'], negative=['kvinde'], topn=5))"
   ]
  },
  {
   "source": [
    "## Odd-one-out Detection\n",
    "This works by:\n",
    "- take the mean of all the word-embeddings\n",
    "- calculate the cosine-distance (similarity) from that center to each word\n",
    "- return the most dissimilar word (i.e. the one with the highest cosine-distance from that mean vector)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'emil'"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "word_embeddings.doesnt_match(\"kenneth lasse emil mikkel\".split())"
   ]
  },
  {
   "source": [
    "## Other ways word embeddings have been used\n",
    "\n",
    "shift in word meaning over time\n",
    "![](https://ruder.io/content/images/size/w2000/2017/10/semantic_change.png)\n",
    "\n",
    "Cross lingual word embeddings:\n",
    "![](https://s3.ap-south-1.amazonaws.com/techleerimages/771a4957-7fb8-4ddd-ba04-4fa73187e5f1.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exercises:\n",
    "These exercises is made for the Danish word embedding but feel free to use the embeddings by google instead.\n",
    "\n",
    "- What is to is to woman (\"kvinde\") what man (\"mand\") to doctor (\"læge\") is this problematic?\n",
    "- Discuss how you would find pluralis of a Danish word\n",
    "    - Find pluralis of 3 danish words using word embeddings\n",
    "- Discuss how you would find the antonym of a Danish word\n",
    "    - Find 3\n",
    "- Examine 3 word with multiple meaning, how well does word embeddings hande these?\n",
    "- Download this tagged [data](https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-da-32.txt). This is from a Danish sentiment lexicon from AFINN tagged by Finn A. Nielsen (A Danish NLP researcher). Discuss how you could expand this lexicon using word embeddings and test out if your assumptions are correct.\n",
    "- Can you use odd one out detection on AFINN's sentiment lexicon to check for errors in the dictionary? I.e. words which are tagged as positive but are in fact not?\n",
    "- Read the following exercise. Is there other entities which you could classify using word embeddings. Think first entities which we have talkedd about in the litterature and the think about more general concepts are word pocess (such as )\n",
    "\n",
    "- train a model using word embeddings instead of to predict positive and negative words. I will suggest the following steps, but there is other ways of doing this feel free to diverge from these instructions:\n",
    "\n",
    "    - 1) Extract positive, negative and neutral words from this (You will have to choose a reasonable cut-offs)\n",
    "\n",
    "    - 2) Train a classifier using scikit-learn which takes a word embedding as input and a outputs whether a word its sentiment\n",
    "\n",
    "    - 3) calculate the performance metrics on a hold out test set (how well does it perform on unseen words?)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('gynækolog', 0.8005026578903198), ('praktiserende', 0.7944498062133789), ('lægen', 0.7603233456611633), ('terapeut', 0.7516137957572937), ('gynækologisk', 0.7430381774902344)]\n"
     ]
    }
   ],
   "source": [
    "# doctor - man + woman\n",
    "print(word_embeddings.most_similar(positive=['læge', 'kvinde'], negative=['mand'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('friskpresset', 0.6577655076980591), ('beanboozled', 0.6317614316940308), ('instantkaffe', 0.6234259009361267), ('theer', 0.6210190057754517), ('juice.', 0.6203752756118774)]\n"
     ]
    }
   ],
   "source": [
    "# plural - singular <-- pluralis embedding \n",
    "print(word_embeddings.most_similar(positive=['kvinder', 'kop'], negative=['kvinde'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('juleengel', 0.6133499145507812), ('froh', 0.6062278747558594), ('nisse', 0.603308916091919), ('bords', 0.599431037902832), ('tinsoldat', 0.5920867919921875)]\n"
     ]
    }
   ],
   "source": [
    "# word - analogy <-- analogy embedding (then word + analogy embedding => analogy)\n",
    "print(word_embeddings.most_similar(positive=['god', 'engel'], negative=['dårlig'], topn=5))"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}