{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Let's start of with a where we left of last time. Simple classification using a bag-of-words. Today we will only be using the multinomial naive bayes to keep the focus on the vectorization.\n",
    "\n",
    "** Task go through the script in studygroups read through the script, make sure you understanding the working as you go through it.**\n",
    "\n",
    "if you find any if this uninterpretable and or difficult, remember that I will be in the class or on zoom if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../class_05/imdb\n"
     ]
    }
   ],
   "source": [
    "# add classification script to path (as well as data)\n",
    "import sys\n",
    "import os\n",
    "path = os.path.join(\"..\", \"class_05\")  # create path - will be different depending on mac vs windows\n",
    "\n",
    "sys.path.append(path)  # add path\n",
    "\n",
    "# create path for imdb dataset\n",
    "imdbpath = os.path.join(\"..\", \"class_05\", \"imdb\")\n",
    "print(imdbpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model obtained a performance accuracy of 0.81\n"
     ]
    }
   ],
   "source": [
    "from classification import read_imdb\n",
    "\n",
    "from sklearn.pipeline import Pipeline #not sure. \n",
    "from sklearn.model_selection import train_test_split #splitting. \n",
    "from sklearn.feature_extraction.text import CountVectorizer #just count (not tf-ifd)\n",
    "from sklearn.naive_bayes import MultinomialNB #multinomial. \n",
    "\n",
    "# read data\n",
    "imdb = read_imdb(imdbpath)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(imdb.text, imdb.tag)\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), \n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "# train model\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "# estimate performance\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "acc = sum(predictions == y_test)/len(y_test)\n",
    "print(f\"Our model obtained a performance accuracy of {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying with our own function\n",
    "def lemmatize_stanza(tokenlist, return_df=False):\n",
    "    \"\"\"\n",
    "    tokenlist (list): A list of tokens\n",
    "\n",
    "    lemmatize a tokenlist using stanza\n",
    "    \"\"\"\n",
    "\n",
    "    import stanza\n",
    "\n",
    "    nlp = stanza.Pipeline(\n",
    "        lang=\"en\", processors=\"tokenize,lemma\", tokenize_pretokenized=True\n",
    "    )\n",
    "    doc = nlp(tokenlist)\n",
    "\n",
    "    res = [\n",
    "        (word.lemma) for n_sent, sent in enumerate(doc.sentences) for word in sent.words\n",
    "    ]\n",
    "\n",
    "    if return_df:\n",
    "        import pandas as pd\n",
    "\n",
    "        return pd.DataFrame(res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-09 09:59:17 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-10-09 09:59:17 INFO: Use device: cpu\n",
      "2020-10-09 09:59:17 INFO: Loading: tokenize\n",
      "2020-10-09 09:59:17 INFO: Loading: lemma\n",
      "2020-10-09 09:59:17 INFO: Done loading processors!\n",
      "2020-10-09 09:59:17 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-10-09 09:59:17 INFO: Use device: cpu\n",
      "2020-10-09 09:59:17 INFO: Loading: tokenize\n",
      "2020-10-09 09:59:17 INFO: Loading: lemma\n",
      "2020-10-09 09:59:17 INFO: Done loading processors!\n",
      "2020-10-09 09:59:17 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| lemma     | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-10-09 09:59:17 INFO: Use device: cpu\n",
      "2020-10-09 09:59:17 INFO: Loading: tokenize\n",
      "2020-10-09 09:59:17 INFO: Loading: lemma\n",
      "2020-10-09 09:59:17 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nlp', 'be', 'very', 'very', 'fun']\n",
      "['nlp', 'teacher', 'be', 'fun']\n",
      "['a', 'teacher', 'be', 'a', 'person']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1, 0, 0, 2],\n",
       "        [0, 1, 1, 1, 0, 1, 0],\n",
       "        [2, 1, 0, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With lemmatization instead. \n",
    "doc = [\"NLP is very very fun\", \n",
    "       \"NLP teachers are fun\",\n",
    "       \"a teacher is a person\"]\n",
    "\n",
    "from text_preprocessor_VMP import Text\n",
    "\n",
    "def lemma_wrapper(txt): \n",
    "    TextObject = Text(txt)\n",
    "    lemma = TextObject.lemmatize_stanza()\n",
    "    print(lemma)\n",
    "    return lemma \n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=lemma_wrapper, lowercase=False)\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now let is try to add our own tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'very', 'very', 'fun']\n",
      "<class 'list'>\n",
      "['NLP', 'teachers', 'are', 'fun']\n",
      "<class 'list'>\n",
      "['a', 'teacher', 'is', 'a', 'person']\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 1, 1, 0, 0, 0, 2],\n",
       "        [1, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "        [0, 2, 0, 0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# a simple test set:\n",
    "doc = [\"NLP is very very fun\", \n",
    "       \"NLP teachers are fun\",\n",
    "       \"a teacher is a person\"]\n",
    "\n",
    "from text_preprocessor import Text #importing some class?\n",
    "\n",
    "# define a wrapper function which only returns tokens and handles list\n",
    "def tokenization_wrapper(txt):\n",
    "    TextObject = Text(txt)\n",
    "    TextObject.tokenize(method=\"nltk\") #tokenization. \n",
    "    tokens = TextObject.get_tokens()\n",
    "    print(tokens)\n",
    "    print(type(tokens))\n",
    "    return tokens\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenization_wrapper, lowercase=False)\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()\n",
    "# TASK: Change the above code use your lemmatization function instead of pure tokenization. What would you expect would change?\n",
    "# But - didn't we do the lemmatization with stanza?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You can make this matrix more visually appealing (and understandable) quite well using this trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLP</th>\n",
       "      <th>a</th>\n",
       "      <th>are</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>person</th>\n",
       "      <th>teacher</th>\n",
       "      <th>teachers</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NLP  a  are  fun  is  person  teacher  teachers  very\n",
       "0    1  0    0    1   1       0        0         0     2\n",
       "1    1  0    1    1   0       0        0         1     0\n",
       "2    0  2    0    0   1       1        1         0     0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(bow.todense(), columns=vectorizer.get_feature_names())\n",
    "# Notice that teacher and teachers are two different tokens is this problematic?\n",
    "# TASK: the visualization code is a bit unclear to understand. Make a function on the form:\n",
    "# vizualize(bow, vectorizer)\n",
    "# which produces the table below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 1)\t2\n",
      "  (2, 6)\t1\n",
      "  (2, 5)\t1\n",
      "[[1 0 0 1 1 0 0 0 2]\n",
      " [1 0 1 1 0 0 0 1 0]\n",
      " [0 2 0 0 1 1 1 0 0]]\n",
      "['NLP', 'a', 'are', 'fun', 'is', 'person', 'teacher', 'teachers', 'very']\n"
     ]
    }
   ],
   "source": [
    "print(bow)\n",
    "print(bow.todense())\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "We now want to look a bit more into what goes on in the vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using 1-grams and bigrams:\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3)) # 2 and 3? does anything change?\n",
    "\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()\n",
    "# TASK change it to only use 2-grams and 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/victormp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# to use stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwordlist = stopwords.words('english')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwordlist)\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()\n",
    "# TASK what words were removed using the stopwordlist, might these be meaningful?\n",
    "print(stopwordlist) # most - shouldn't - not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1, 0, 0, 0, 2],\n",
       "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to remove stopwords\n",
    "vectorizer = CountVectorizer(max_df=0.9) # remove words which appear in 90% of the documents\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 1, 0, 0, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make a binary classification (is the word there or not)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1],\n",
       "        [1, 0, 1],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words which appear less than min_df (i.e. words which appear only once)\n",
    "vectorizer = CountVectorizer(min_df=2)\n",
    "bow = vectorizer.fit_transform(doc)\n",
    "bow.todense()\n",
    "\n",
    "# min_df can also be a number from 0-1. If so it is a percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tf-Idf\n",
    "\n",
    "tf-idf for a specific term (word) $t$ in a given document $d$, as you probably know from the lectures, is calculated using the\n",
    "\n",
    "$tf-idf(t, d) = tf(t, d) * idf(t)$\n",
    "\n",
    "Where $tf$ is the frequency of term $t$ in the document $d$.and $idf$ is the inverse document frequency of the term $t$. In sklearn $idf$ is calculated as follows:\n",
    "\n",
    "$idf(t) = log \\frac{1 + n}{1 + df(t)} + 1$\n",
    "\n",
    "it is then normalized using the l2 norm (the euclidian norm for all math people out there). It simply makes sure it the length of vector for the document is one. It is calculated as follows:\n",
    "\n",
    "$\\frac{1}{\\sqrt{v_1^2 + v^2 + ...}}$\n",
    "\n",
    "As an example the norm of v would be:\n",
    "\n",
    "$\n",
    "  \\begin{align}\n",
    "    ||v|| &= ||\\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "         \\end{bmatrix}|| = \\frac{1}{\\sqrt{1 + 1 + 0}} = \\frac{1}{\\sqrt{2}} \n",
    "  \\end{align}\n",
    "  $\n",
    "\n",
    "\n",
    "The tf idf in sklearn uses the default parameters:\n",
    "```python\n",
    "norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False\n",
    "```\n",
    "\n",
    "where `norm` toggles what type of normalization it should use. `use_idf` toggles whether is should multiple tf with idf not not.\n",
    "`smooth_idf` the is the `+1` fraction. i.e. setting it to false would do the following: \n",
    "\n",
    "$idf(t) = log \\frac{n}{df(t)} + 1$\n",
    "\n",
    "This is the same as the add one smoothing the naive bayes assignment.\n",
    "\n",
    "Interestingly tf-idf can also be derived using information theory as **the amount of information gained seeing the word weighted by your probability of seeing it**. It is thus reasonable that it is good for performance, not only that, but it also fits nicely into cognitive theories such as bayesian brain and the free energy principles.\n",
    "\n",
    "Read a lot more on tf-idf in sklearn [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
    "\n",
    "There is two ways to call do tf-idf in scikit-learn. Either using the `TfidfVectorizer` or transforming the `CountVectorizer` using `TfidfTransformer`. I will just be using the first one for simplicity. You can pass `TfidfVectorizer` all the same arguments as we have used for the `CountVectorizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>fun</th>\n",
       "      <th>is</th>\n",
       "      <th>nlp</th>\n",
       "      <th>person</th>\n",
       "      <th>teacher</th>\n",
       "      <th>teachers</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317570</td>\n",
       "      <td>0.31757</td>\n",
       "      <td>0.317570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.47363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are       fun       is       nlp    person   teacher  teachers  \\\n",
       "0  0.000000  0.317570  0.31757  0.317570  0.000000  0.000000  0.000000   \n",
       "1  0.562829  0.428046  0.00000  0.428046  0.000000  0.000000  0.562829   \n",
       "2  0.000000  0.000000  0.47363  0.000000  0.622766  0.622766  0.000000   \n",
       "\n",
       "       very  \n",
       "0  0.835133  \n",
       "1  0.000000  \n",
       "2  0.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "ti = tfidf.fit_transform(doc)\n",
    "ti.todense()\n",
    "pd.DataFrame(ti.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Text' object has no attribute 'lemmatize_stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-981a2c981da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         )\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \"\"\"\n\u001b[1;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-2ab16d607801>\u001b[0m in \u001b[0;36mlemma_wrapper\u001b[0;34m(txt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemma_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mTextObject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize_stanza\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Text' object has no attribute 'lemmatize_stanza'"
     ]
    }
   ],
   "source": [
    "# we can add all the previous stuff as well\n",
    "tfidf = TfidfVectorizer(#tokenizer=tokenization_wrapper,\n",
    "                        tokenizer=lemma_wrapper, #why does this not work?\n",
    "                        stop_words=stopwordlist,\n",
    "                        min_df=1,\n",
    "                        max_df=0.8,\n",
    "                        ngram_range=(1,2),\n",
    "                        lowercase=False,\n",
    "                        binary=False\n",
    "                        )\n",
    "ti = tfidf.fit_transform(doc)\n",
    "ti.todense()\n",
    "pd.DataFrame(ti.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "As the matrix for language classification typically becomes quite large an idea is to reduce the dimensionality of the matrix. Typically used techniques reducing dimensionality in data include Principal component analysis (PCA) or singular value decompositions (SVD). Note that PCA was introduced 2nd semester Experimental Methods so people should be familiar (alternatively there is also a video in the highly recommended readings for class). PCA is also used in computing the intelligence metric, g-factor. PCA is calculated using SVD which, for the mathematically inclined, is the generalization of eigendecompositions (finding eigenvalues and eigenvectors) to non-diagnizable matrices. \n",
    "\n",
    "Other approaches include latent semantic analysis (LSA) which similar to PCA is build upon SVD. LSA was later reinterpreted in a probalistic framework (pLSA) which was generalized into Latent Dirichlet Allocation (LDA) which is what is the topic model we will go into in week 44. LSA is also more efficient for tf-idf and tf matrices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the explained variance in the dataset of each of the three components:\n",
      "[0.503 0.437 0.   ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA #principal component. \n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "ti = tfidf.fit_transform(doc)\n",
    "pca = PCA(n_components=3) # should be bigger for real data.\n",
    "output = pca.fit_transform(ti.todense())  # it is required to make the tf-idf matrix (ti) to dense to do PCA, but this is computionally heavy (thus the LSA)\n",
    "\n",
    "print(\"the explained variance in the dataset of each of the three components:\")\n",
    "print(pca.explained_variance_.round(3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the explained variance in the dataset of each of the three components:\n",
      "[0.02 0.32 0.28]\n",
      "0.6261949130129313\n",
      "\n",
      "the reduced matrix:\n",
      "[[ 0.75253981  0.          0.65854676]\n",
      " [ 0.64398422 -0.51739333 -0.56354988]\n",
      " [ 0.38935908  0.85574771 -0.3407277 ]]\n",
      "\n",
      "the previous matrix:\n",
      "[[0.         0.         0.21311356 0.21311356 0.         0.28021872\n",
      "  0.28021872 0.21311356 0.28021872 0.28021872 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56043744 0.28021872 0.28021872 0.28021872]\n",
      " [0.35013871 0.35013871 0.26628951 0.         0.         0.\n",
      "  0.         0.26628951 0.         0.         0.35013871 0.35013871\n",
      "  0.         0.         0.         0.         0.35013871 0.35013871\n",
      "  0.35013871 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.32200242 0.42339448 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.42339448 0.42339448 0.42339448 0.42339448 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD as LSA\n",
    "lsa = LSA(n_components=3) # should be bigger for real data\n",
    "output = lsa.fit_transform(ti)  # notice the lack of todense\n",
    "\n",
    "print(\"the explained variance in the dataset of each of the three components:\")\n",
    "print(lsa.explained_variance_.round(2)) \n",
    "print(sum(lsa.explained_variance_)) \n",
    "\n",
    "print(\"\\nthe reduced matrix:\")\n",
    "print(output)\n",
    "\n",
    "print(\"\\nthe previous matrix:\")\n",
    "print(ti.todense())\n",
    "# notice how many of the entries in the tf-idf (similar to the bow matrices) contain a lot of zeros, which indicate that the information can typically be reduced dramatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tasks\n",
    "1) putting it all together. Create a pipeline which does the both the vectorization, dimensionality reduction and the classification using the imdb dataset. Set the original test set aside as a validation set and make a new train test split.\n",
    "here the train is for training on. Test is for testing performance and the validation set is to validate performance (which we will do in 7)\n",
    "\n",
    "2) Change the preprocessing to get a better performance (don't spent to long here, just a little bit of tuning)\n",
    "\n",
    "3) Apply the same model to the 20 newsgroup data (see next block on how to read it in). Again split up the train set into a train and test set. leave the actual test set i.e. validation set for 7 (loaded in by setting `subset=\"test\"`).\n",
    "\n",
    "4) apply the same model to the sms spam data from class 4. (spam.csv). Remember to split it into a validation set and a train set and then again split the train set into a train and a test set.\n",
    "\n",
    "5) When you finetune your model now do you see that you preprocessing choices you made were only good for specific types of dataset  or did they generalize well? Can you find general trends with works better than others? E.g. Is stopwords a good idea? Does tf-idf outperform bag of words? Found out 5 of such 'truths' and post then on element\n",
    "\n",
    "6) Finally finetune yours models. Optimize the vectorization as much as you want to. Use cross validation or grid search on the train/test set. Until you are satisfied. (it does not have to be the same preprocessings steps for all three dataset anymore)\n",
    "\n",
    "7) Finally apply your model to validation sets and report your models performance on element. It should have the form:\n",
    "\n",
    "*\"We obtained a performance of XX% on validation set of (the dataset) using (your model specifications)...\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classification script to path (as well as data)\n",
    "import sys\n",
    "import os\n",
    "from classification import read_imdb\n",
    "from sklearn.pipeline import Pipeline #not sure. \n",
    "from sklearn.model_selection import train_test_split #splitting. \n",
    "from sklearn.feature_extraction.text import CountVectorizer #just count (not tf-ifd)\n",
    "from sklearn.naive_bayes import MultinomialNB #multinomial.\n",
    "\n",
    "## pathing: \n",
    "path = os.path.join(\"..\", \"class_05\")  # create path - will be different depending on mac vs windows\n",
    "sys.path.append(path)  # add path\n",
    "\n",
    "# create path for imdb dataset\n",
    "imdbpath = os.path.join(\"..\", \"class_05\", \"imdb\")\n",
    "print(imdbpath)\n",
    "\n",
    "# read data\n",
    "imdb = read_imdb(imdbpath)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(imdb.text, imdb.tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## reading in the 20 newsgroup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>targets</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier)\\nSubj...</td>\n",
       "      <td>1</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...</td>\n",
       "      <td>3</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...</td>\n",
       "      <td>3</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>3</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  targets  \\\n",
       "0  From: sd345@city.ac.uk (Michael Collier)\\nSubj...        1   \n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...        1   \n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...        3   \n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...        3   \n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...        3   \n",
       "\n",
       "                 category  \n",
       "0  soc.religion.christian  \n",
       "1  soc.religion.christian  \n",
       "2                 sci.med  \n",
       "3                 sci.med  \n",
       "4                 sci.med  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# select which categories to load\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', # only load the training set\n",
    "                                  categories=categories)\n",
    "\n",
    "# transform the data (not necessary but nice)\n",
    "df = pd.DataFrame({\"text\": twenty_train.data, \"targets\": twenty_train.target})\n",
    "df[\"category\"] = df.targets.apply(lambda x: categories[x])\n",
    "\n",
    "# examine the data (first five rows)\n",
    "df.head()\n",
    "\n",
    "# SUGGESTION: loading dataset like this can be hard to read later on. Wrapping it all into a function e.g. read_20_news make the code easy to read an easy to reuse later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
